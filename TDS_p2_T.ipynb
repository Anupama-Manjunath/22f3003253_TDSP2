{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "446c64cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage: python script.py <file_path>\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "1",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 1\n"
     ]
    }
   ],
   "source": [
    "# /// script\n",
    "# requires-python = \">=3.11\"\n",
    "# dependencies = [\n",
    "#   \"seaborn\",\n",
    "#   \"pandas\",\n",
    "#   \"matplotlib\",\n",
    "#   \"httpx\",\n",
    "#   \"chardet\",\n",
    "#   \"ipykernel\",\n",
    "#   \"openai\",\n",
    "#   \"numpy\",\n",
    "#   \"scipy\",\n",
    "# ]\n",
    "# ///\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import httpx\n",
    "import chardet\n",
    "from pathlib import Path\n",
    "import asyncio\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Constants\n",
    "API_URL = \"https://aiproxy.sanand.workers.dev/openai/v1/chat/completions\"\n",
    "\n",
    "# Ensure token is retrieved from environment variable\n",
    "def get_token():\n",
    "    try:\n",
    "        return os.environ[\"AIPROXY_TOKEN\"]\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Environment variable '{e.args[0]}' not set.\")\n",
    "        raise\n",
    "\n",
    "async def load_data(file_path):\n",
    "    \"\"\"Load CSV data with encoding detection.\"\"\"\n",
    "    if not os.path.isfile(file_path):\n",
    "        raise FileNotFoundError(f\"Error: File '{file_path}' not found.\")\n",
    "\n",
    "    with open(file_path, 'rb') as f:\n",
    "        result = chardet.detect(f.read())\n",
    "    encoding = result['encoding']\n",
    "    print(f\"Detected file encoding: {encoding}\")\n",
    "    return pd.read_csv(file_path, encoding=encoding)\n",
    "\n",
    "async def async_post_request(headers, data):\n",
    "    \"\"\"Async function to make HTTP requests.\"\"\"\n",
    "    async with httpx.AsyncClient() as client:\n",
    "        try:\n",
    "            response = await client.post(API_URL, headers=headers, json=data, timeout=30.0)\n",
    "            response.raise_for_status()\n",
    "            return response.json()['choices'][0]['message']['content']\n",
    "        except httpx.HTTPStatusError as e:\n",
    "            print(f\"HTTP error occurred: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            print(f\"Error during request: {e}\")\n",
    "            raise\n",
    "\n",
    "async def generate_narrative(analysis, token, file_path):\n",
    "    \"\"\"Generate narrative using LLM.\"\"\"\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are a data analyst. Provide a detailed narrative based on the following data analysis results for the file '{file_path.name}':\\n\\n\"\n",
    "        f\"Column Names & Types: {list(analysis['summary'].keys())}\\n\\n\"\n",
    "        f\"Summary Statistics: {analysis['summary']}\\n\\n\"\n",
    "        f\"Missing Values: {analysis['missing_values']}\\n\\n\"\n",
    "        f\"Correlation Matrix: {analysis['correlation']}\\n\\n\"\n",
    "        \"Please provide insights into trends, outliers, anomalies, or patterns. \"\n",
    "        \"Suggest further analyses like clustering or anomaly detection. \"\n",
    "        \"Discuss how these trends may impact future decisions.\"\n",
    "    )\n",
    "\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    return await async_post_request(headers, data)\n",
    "\n",
    "async def analyze_data(df, token):\n",
    "    \"\"\"Use LLM to suggest and perform data analysis.\"\"\"\n",
    "    if df.empty:\n",
    "        raise ValueError(\"Error: Dataset is empty.\")\n",
    "\n",
    "    # Enhanced prompt for better LLM analysis suggestions\n",
    "    prompt = (\n",
    "        f\"You are a data analyst. Given the following dataset information, provide an analysis plan and suggest useful techniques:\\n\\n\"\n",
    "        f\"Columns: {list(df.columns)}\\n\"\n",
    "        f\"Data Types: {df.dtypes.to_dict()}\\n\"\n",
    "        f\"First 5 rows of data:\\n{df.head()}\\n\\n\"\n",
    "        \"Suggest data analysis techniques, such as correlation, regression, anomaly detection, clustering, or others. \"\n",
    "        \"Consider missing values, categorical variables, and scalability.\"\n",
    "    )\n",
    "\n",
    "    headers = {\n",
    "        'Authorization': f'Bearer {token}',\n",
    "        'Content-Type': 'application/json'\n",
    "    }\n",
    "    data = {\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        suggestions = await async_post_request(headers, data)\n",
    "    except Exception as e:\n",
    "        suggestions = f\"Error fetching suggestions: {e}\"\n",
    "\n",
    "    print(f\"LLM Suggestions: {suggestions}\")\n",
    "\n",
    "    # Basic analysis (summary statistics, missing values, correlations)\n",
    "    numeric_df = df.select_dtypes(include=['number'])\n",
    "    analysis = {\n",
    "        'summary': df.describe(include='all').to_dict(),\n",
    "        'missing_values': df.isnull().sum().to_dict(),\n",
    "        'correlation': numeric_df.corr().to_dict() if not numeric_df.empty else {}\n",
    "    }\n",
    "\n",
    "    # Hypothesis testing example (if 'A' and 'B' columns exist)\n",
    "    if 'A' in df.columns and 'B' in df.columns:\n",
    "        t_stat, p_value = stats.ttest_ind(df['A'].dropna(), df['B'].dropna())\n",
    "        analysis['hypothesis_test'] = {\n",
    "            't_stat': t_stat,\n",
    "            'p_value': p_value\n",
    "        }\n",
    "\n",
    "    print(\"Data analysis complete.\")\n",
    "    return analysis, suggestions\n",
    "\n",
    "async def visualize_data(df, output_dir):\n",
    "    \"\"\"Generate and save visualizations.\"\"\"\n",
    "    sns.set(style=\"whitegrid\")\n",
    "    numeric_columns = df.select_dtypes(include=['number']).columns\n",
    "\n",
    "    # Select main columns for distribution based on importance\n",
    "    selected_columns = numeric_columns[:3] if len(numeric_columns) >= 3 else numeric_columns\n",
    "\n",
    "    # Ensure output directory exists\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Enhanced visualizations (distribution plots, heatmap)\n",
    "    for column in selected_columns:\n",
    "        plt.figure(figsize=(6, 6))\n",
    "        sns.histplot(df[column].dropna(), kde=True, color='skyblue')\n",
    "        plt.title(f'Distribution of {column}')\n",
    "        file_name = output_dir / f'{column}_distribution.png'\n",
    "        plt.savefig(file_name, dpi=100)\n",
    "        print(f\"Saved distribution plot: {file_name}\")\n",
    "        plt.close()\n",
    "\n",
    "    if len(numeric_columns) > 1:\n",
    "        plt.figure(figsize=(8, 8))\n",
    "        corr = df[numeric_columns].corr()\n",
    "        sns.heatmap(corr, annot=True, cmap='coolwarm', square=True)\n",
    "        plt.title('Correlation Heatmap')\n",
    "        file_name = output_dir / 'correlation_heatmap.png'\n",
    "        plt.savefig(file_name, dpi=100)\n",
    "        print(f\"Saved correlation heatmap: {file_name}\")\n",
    "        plt.close()\n",
    "\n",
    "async def save_narrative_with_images(narrative, output_dir):\n",
    "    \"\"\"Save narrative to README.md and embed image links.\"\"\"\n",
    "    readme_path = output_dir / 'README.md'\n",
    "    image_links = \"\\n\".join(\n",
    "        [f\"![{img.name}]({img.name})\" for img in output_dir.glob('*.png')]\n",
    "    )\n",
    "    with open(readme_path, 'w') as f:\n",
    "        f.write(narrative + \"\\n\\n\" + image_links)\n",
    "    print(f\"Narrative successfully written to {readme_path}\")\n",
    "\n",
    "async def main(file_path):\n",
    "    print(\"Starting autolysis process...\")\n",
    "\n",
    "    # Ensure input file exists\n",
    "    file_path = Path(file_path)\n",
    "    if not file_path.is_file():\n",
    "        print(f\"Error: File '{file_path}' does not exist.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load token\n",
    "    try:\n",
    "        token = get_token()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Load dataset\n",
    "    try:\n",
    "        df = await load_data(file_path)\n",
    "    except FileNotFoundError as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "    print(\"Dataset loaded successfully.\")\n",
    "\n",
    "    # Analyze data with LLM insights\n",
    "    print(\"Analyzing data...\")\n",
    "    try:\n",
    "        analysis, suggestions = await analyze_data(df, token)\n",
    "    except ValueError as e:\n",
    "        print(e)\n",
    "        sys.exit(1)\n",
    "\n",
    "    print(f\"LLM Analysis Suggestions: {suggestions}\")\n",
    "\n",
    "    # Create output directory\n",
    "    output_dir = Path(file_path.stem)  # Create a directory named after the dataset\n",
    "    output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "    # Generate visualizations with LLM suggestions\n",
    "    print(\"Generating visualizations...\")\n",
    "    await visualize_data(df, output_dir)\n",
    "\n",
    "    # Generate narrative\n",
    "    print(\"Generating narrative using LLM...\")\n",
    "    narrative = await generate_narrative(analysis, token, file_path)\n",
    "\n",
    "    if narrative != \"Narrative generation failed due to an error.\":\n",
    "        await save_narrative_with_images(narrative, output_dir)\n",
    "    else:\n",
    "        print(\"Narrative generation failed.\")\n",
    "\n",
    "# Execute script\n",
    "if __name__ == \"__main__\":\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: python script.py <file_path>\")\n",
    "        sys.exit(1)\n",
    "    asyncio.run(main(sys.argv[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dbc7d38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting httpx\n",
      "  Downloading httpx-0.28.1-py3-none-any.whl (73 kB)\n",
      "     ---------------------------------------- 73.5/73.5 kB 4.2 MB/s eta 0:00:00\n",
      "Requirement already satisfied: idna in c:\\users\\0310a\\anaconda3\\lib\\site-packages (from httpx) (3.4)\n",
      "Collecting httpcore==1.*\n",
      "  Downloading httpcore-1.0.7-py3-none-any.whl (78 kB)\n",
      "     ---------------------------------------- 78.6/78.6 kB 4.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: anyio in c:\\users\\0310a\\anaconda3\\lib\\site-packages (from httpx) (3.5.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\0310a\\anaconda3\\lib\\site-packages (from httpx) (2023.5.7)\n",
      "Collecting h11<0.15,>=0.13\n",
      "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "     ---------------------------------------- 58.3/58.3 kB 3.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\0310a\\anaconda3\\lib\\site-packages (from anyio->httpx) (1.2.0)\n",
      "Installing collected packages: h11, httpcore, httpx\n",
      "Successfully installed h11-0.14.0 httpcore-1.0.7 httpx-0.28.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install httpx\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b619bb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "eyJhbGciOiJIUzI1NiJ9.eyJlbWFpbCI6IjIyZjMwMDMyNTNAZHMuc3R1ZHkuaWl0bS5hYy5pbiJ9.lTDkW3eecrVem7cJCZtBFhvNdwTp0G_9s4JQ1Wgeg9k"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
